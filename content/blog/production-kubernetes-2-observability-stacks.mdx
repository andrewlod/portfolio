---
id: 'production-kubernetes-2-observability-stacks'
title: 'Production-ready Kubernetes Part 2 - Observability Stacks'
subtitle: 'Choosing Architecture Over Hype'
category: 'DevOps'
date: '2026-02-26 21:00:00'
featured: false

impact: {}

metrics: {}

technologies:
  - Elastic stack
  - Elasticsearch
  - ELK
  - Grafana
  - LGTM
  - Logs
  - Loki
  - Metrics
  - Mimir
  - Traces
  - Observability
  - Opensearch
  - OpenTelemetry
  - Prometheus
  - SLA
  - SLO
  - SLI
  - Tempo
---

## The Stack Before the Strategy Problem

Most Kubernetes observability implementations donâ€™t fail because of tooling limitations.

They fail because of **lack of intent.**

Teams often adopt an observability stack the same way they adopt many other technologies:
- â€œEveryone uses itâ€
- â€œIt integrates with Xâ€
- â€œWe inherited itâ€
- â€œIt looked good in a demoâ€

The result?

Dashboards exist. Logs flow. Metrics accumulate. Traces are enabled.
- Yet during incidents:
- Root cause analysis is slow
- Signals conflict
- Data is fragmented
- Costs spiral
- Alert fatigue grows

The issue is rarely missing telemetry.

It is **misaligned observability architecture.**

In production Kubernetes environments â€” where systems are distributed, ephemeral, and failure-prone by design â€” your stack choice directly impacts:
- âœ” Mean time to detect (MTTD)
- âœ” Mean time to resolve (MTTR)
- âœ” Cost efficiency
- âœ” Cognitive load on engineers
- âœ” Long-term scalability

Choosing an observability stack is therefore not a tooling decision.

It is an **architectural decision.**

---

## The Three Dominant Approaches in 2026

While the ecosystem is vast, most production environments gravitate toward three patterns:
1. **The LGTM Stack (Loki, Grafana, Tempo, Mimir)**
2. **The Elastic Stack**
3. **OpenTelemetry-Centric Architectures**

Each reflects a different philosophy about:
- Data storage
- Query patterns
- Integration strategy
- Operational overhead
- Vendor lock-in
- Cost model

Understanding these differences is essential.

---

## ğŸ§— The LGTM Stack
### Unified Experience & Cloud-Native Efficiency

**Components:** Loki (logs), Grafana (visualization), Tempo (traces), Mimir(+ Prometheus) (metrics)

The LGTM stack has gained strong adoption among cloud-native teams because it emphasizes:
- âœ” Tight integration
- âœ” Consistent UI/UX
- âœ” Kubernetes-native workflows
- âœ” Resource efficiency
- âœ” Lower operational complexity

### Core Philosophy

Instead of treating logs, metrics, and traces as separate universes, LGTM promotes:

<Callout type="info">
ğŸ‘‰ â€œSingle pane of glass observabilityâ€
</Callout>

Engineers can pivot between:

Metric â†’ Trace â†’ Log â†’ Dashboard

without context switching across platforms.

### Where LGTM Excels

âœ… **Cloud-native microservices** - Designed for Kubernetes patterns and ephemeral workloads.

âœ… **Teams prioritizing operational simplicity** - Lower cognitive friction, fewer disconnected tools.

âœ… **Cost-conscious environments** - Loki and Tempo are optimized for efficient storage patterns.

âœ… **Correlation-heavy workflows** - Grafana enables smooth cross-signal navigation.

### Typical Strengths
- âœ” Unified visualization layer
- âœ” Lower infrastructure footprint (vs heavy OLAP engines)
- âœ” Native Kubernetes integration
- âœ” Easier onboarding for engineers
- âœ” Strong developer experience

### Common Pitfalls
- âŒ Assuming â€œunified UI = unified strategyâ€
- âŒ Poor metric design â†’ noisy dashboards
- âŒ Cardinality mismanagement
- âŒ Treating Grafana as decoration rather than diagnosis

In summary, focusing purely on telemetry mechanics while ignoring business context is one of the fastest ways to misuse LGTM â€” or any observability stack.

### Bad Implementation Example

**Symptoms:**
- Hundreds of dashboards with no real connection to the business itself
- Metrics with pod_name, pod_ip, container_id labels (really depends on use case, but usually you don't want these there)
- Logs rarely queried
- Traces enabled but ignored

**Outcome:**
- Observability theater.
- High ingestion costs.
- Low diagnostic value.

### Good Implementation Example

**Characteristics:**
- âœ” Metrics designed around SLIs
- âœ” Cardinality controlled intentionally
- âœ” Logs structured & correlated via trace_id
- âœ” Traces sampled strategically
- âœ” Dashboards built for decisions, not aesthetics
- âœ” Incident runbooks linked to dashboards and concrete steps to be taken

**Outcome:**
- Fast triage
- Clear signal relationships
- Predictable costs

---

## ğŸ” The Elastic Stack
### Deep Search & Analytical Power

Elastic remains dominant in environments where search is the primary investigative tool.

### Core Philosophy

Elastic treats observability data as:

<Callout type="info">
ğŸ‘‰ Observability data treated as analytical, query-first datasets
</Callout>

It shines when you need:
- âœ” Complex queries
- âœ” Deep log forensics
- âœ” Cross-dimensional analysis
- âœ” Massive-scale log analytics

### Where Elastic Excels

âœ… **Log-heavy environments** - Security, compliance, audit trails, debugging via text search.

âœ… **Complex troubleshooting workflows** - Ad-hoc queries across large datasets.

âœ… **Organizations with mature data practices**

âœ… **Very large-scale ingestion**

### Typical Strengths
- âœ” Powerful full-text search
- âœ” Flexible querying
- âœ” Rich analytical capabilities
- âœ” Strong ecosystem maturity
- âœ” Excellent for log-centric investigations

### Common Pitfalls
- âŒ High operational overhead
- âŒ Resource-intensive clusters
- âŒ Cost explosion at scale
- âŒ Fragmented UX if metrics/traces poorly integrated
- âŒ Overengineering for smaller teams

### Bad Implementation Example

**Symptoms:**
- Elastic deployed â€œbecause itâ€™s powerfulâ€
- Minimal query expertise
- Logs dumped unstructured
- Metrics underutilized
- Cluster costs rising

**Outcome:**
- Expensive logging system.
- Low return on complexity.

### Good Implementation Example

**Characteristics:**
- âœ” Structured logs with consistent schemas
- âœ” Query patterns well understood
- âœ” Index lifecycle management tuned
- âœ” Used where analytical (not transactional) search is critical
- âœ” Integrated with metrics/traces intentionally

**Outcome:**
- Exceptional forensic capability.
- High diagnostic precision.

---

## ğŸ”­ OpenTelemetry-Centric Architectures
### Vendor-Neutral Observability

OpenTelemetry (OTel) is less a â€œstackâ€ and more a **strategic layer.**

### Core Philosophy

<Callout type="info">
ğŸ‘‰ Standardize instrumentation, decouple backend choice
</Callout>

Instead of committing early to a vendor:
- Instrument once
- Route anywhere
- Swap backends if needed

The OTel Collector becomes:
- âœ” Pipeline controller
- âœ” Data router
- âœ” Transformation layer
- âœ” Lock-in reducer

### Where OTel Excels
- âœ… **Multi-vendor environments**
- âœ… **Hybrid / multi-cloud architectures**
- âœ… **Organizations avoiding lock-in**
- âœ… **Teams designing long-term portability**

### Typical Strengths
- âœ” Standardized telemetry model
- âœ” Flexible routing
- âœ” Backend independence
- âœ” Future-proof instrumentation
- âœ” Excellent ecosystem momentum

### Common Pitfalls
- âŒ Assuming OTel â€œsolves observabilityâ€ on its own
- âŒ Overcomplicated pipelines
- âŒ Noisy signal routing
- âŒ Lack of backend strategy

OTel is **glue**, not destination.

### Bad Implementation Example

**Symptoms:**
- OTel everywhere
- Data routed to multiple backends
- Adding processors with no clear intention
- No clear ownership
- Signals duplicated
- Engineers confused

**Outcome:**
- Telemetry chaos
- High costs
- Low clarity

### Good Implementation Example

**Characteristics:**
- âœ” OTel as standard instrumentation layer
- âœ” Clear backend selection strategy
- âœ” Sampling policies defined
- âœ” Pipelines minimal & purposeful
- âœ” Used to enable flexibility, not complexity
- âœ” Continuous cost monitoring

**Outcome:**
- Clean architecture
- Portable observability

---

## Architecture-Level Comparison
| Dimension|LGTM|Elastic|OpenTelemetry|
| ----- | ----- | ----- | ----- |
| Philosophy | Unified UX | Deep analytics/search | Vendor-neutral layer |
| Strength | Correlation & simplicity | Log forensics & querying | Flexibility & portability |
| Cost Model | Efficient if designed well | Can grow quickly | Depends on backend |
| Complexity | Moderate | High | Variable |
| Best Fit | Cloud-native teams |Log-heavy / analytical orgs | Multi-backend strategies |

---

## How to Make the Technical Decision

Choosing a stack should begin with **questions, not preferences.**

### 1ï¸âƒ£ What Problem Dominates Your Incidents?

Ask:
- âœ” Are incidents diagnosed via logs?
- âœ” Are SLO breaches your main trigger?
- âœ” Is latency root cause often unclear?
- âœ” Do you need forensic-level search?

Log-centric investigations â†’ Elastic strong fit

Correlation & SLO workflows â†’ LGTM strong fit

### 2ï¸âƒ£ What Is Your Operational Tolerance?

Be honest about:
- âœ” Team expertise
- âœ” Maintenance capacity
- âœ” Infra budget
- âœ” Complexity appetite

Elastic provides power at the cost of **operational load.**

LGTM optimizes for **efficiency and integration.**

### 3ï¸âƒ£ How Important Is Vendor Neutrality?

If you value:
- âœ” Backend flexibility
- âœ” Multi-cloud portability
- âœ” Avoiding lock-in

Then standardize on OpenTelemetry early.

### 4ï¸âƒ£ What Drives Cost in Your Environment?

Observability cost is often dominated by:
- âŒ Log volume
- âŒ High-cardinality metrics
- âŒ Unbounded trace ingestion

Evaluate:
- âœ” Retention policies
- âœ” Sampling strategies
- âœ” Cardinality discipline
- âœ” Data lifecycle management

### 5ï¸âƒ£ Do You Need Unified Experience or Specialized Depth?

Two valid strategies:

Unified Platform Strategy â†’ LGTM-style integration

Specialized Tool Strategy â†’ Elastic + Prometheus + Tempo etc.

Danger arises when teams mix tools **without intentional boundaries.**

---

## A Critical Reality Check

No stack will fix:
- âŒ Poor SLI design
- âŒ Noisy metrics
- âŒ Meaningless logs
- âŒ Random alerts
- âŒ Undefined SLOs

Tools amplify architecture.

They do not replace it.

---

## Conclusion â€” Architecture First, Stack Second

Observability maturity is not measured by:
- Number of dashboards
- Volume of metrics
- Log retention length
- Tracing coverage %

It is measured by:
- âœ” Incident detection speed
- âœ” Diagnostic precision
- âœ” Engineer confidence
- âœ” Cost predictability
- âœ” Signal clarity

The best observability stack is therefore not:
- ğŸ‘‰ â€œMost popularâ€
- ğŸ‘‰ â€œMost powerfulâ€
- ğŸ‘‰ â€œMost featuresâ€

It is:

<Callout type="info">
ğŸ‘‰ The one aligned with your systemâ€™s failure modes, scale, workflows, and constraints
</Callout>

---

## Actionable Steps

If youâ€™re evaluating or redesigning your stack:

âœ… **Step 1 â€” Map Your Incident Patterns** - Where does root cause usually emerge?

âœ… **Step 2 â€” Identify Dominant Signal Type** - Logs? Metrics? Traces?

âœ… **Step 3 â€” Audit Cost Drivers** - Cardinality, ingestion, retention.

âœ… **Step 4 â€” Standardize Instrumentation (OTel strongly recommended)**

âœ… **Step 5 â€” Design for SLOs** - Observability must reflect user experience, not infrastructure vanity metrics.

âœ… **Step 6 â€” Reduce Tool Sprawl** - Every additional platform increases cognitive and operational load.

---

## Final Thought

Observability is not about collecting telemetry.

It is about **designing systems that explain themselves under stress.**

Your stack should serve that goal â€” not become another source of noise.

---

## Related Posts

Production-ready Kubernetes Series:
- [Part 1 - Observability Foundations](https://andrewlod.com/blog/production-kubernetes-1-observability-foundations)
- [Part 2 - Observability Stacks](https://andrewlod.com/blog/production-kubernetes-2-observability-stacks)